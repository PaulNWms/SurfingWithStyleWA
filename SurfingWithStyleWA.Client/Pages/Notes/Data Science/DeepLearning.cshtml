@page "/notes/data-science/deep-learning"

<h1>Deep Learning</h1>

<div class="kaypro10"><pre>Links:
<a target="_blank" href="https://www.manning.com/books/grokking-deep-learning">Grokking Deep Learning</a>

<em>Type</em> of machine learning: supervised vs. unsupervised

<em>Method</em> of machine learning: parametric (trial-and-error) vs. non-parametric ("counting")

Deep learning is a class of parametric model.

# stare at this
weight = 0.5
goal_pred = 0.8
input = 2
alpha = 0.1
for iteration in range(20):
    pred = input * weight
    error = (pred - goal_pred) ** 2
    derivative = input * (pred - goal_pred)
    weight = weight - (alpha * derivative)
    print("Error:" + str(error) + " Prediction:" + str(pred))

Stochastic Gradient Descent updates the weights after each input.
Batch Gradient Descent updates the weights after each batch of input.

# start at this
import numpy as np
np.random.seed(1)
 
def relu(x):
    return (x > 0) * x # returns x if x > 0
                       # return 0 otherwise
 
def relu2deriv(output):
    return output>0 # returns 1 for input > 0
                    # return 0 otherwise
streetlights = np.array( [[ 1, 0, 1 ],
                          [ 0, 1, 1 ],
                          [ 0, 0, 1 ],
                          [ 1, 1, 1 ] ] )
 
walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T
 
alpha = 0.2
hidden_size = 4
 
weights_0_1 = 2*np.random.random((3,hidden_size)) - 1
weights_1_2 = 2*np.random.random((hidden_size,1)) - 1
 
for iteration in range(60):
    layer_2_error = 0
    for i in range(len(streetlights)):
        layer_0 = streetlights[i:i+1]
        layer_1 = relu(np.dot(layer_0,weights_0_1))
        layer_2 = np.dot(layer_1,weights_1_2)
 
        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)
        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])
        layer_1_delta=layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)
        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)
        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)
 
    if(iteration % 10 == 9):
        print("Error:" + str(layer_2_error))

Normalization techniques:
- minibatch
- early stopping
- dropout

Activation functions:
- relu is fast
- sigmoid is often used for output because it squishes the values between 0 and 1
- tanh is often used for middle layers because it squishes the values between -1 and 1

Output activation functions:
- predicting raw data values (like temperature) => no activation
- predicting yes/no probabilities => sigmoid
- predicting "which one" probabilities => softmax

<u>function   forward prop                     back prop delta</u>
- Relu     ones_and_zeros = (input > 0)     mask = output > 0
           output = input*ones_and_zeros    deriv = output * mask
- Sigmoid  output = 1/(1 + np.exp(-input))  deriv = output*(1-output)
- Tanh     output = np.tanh(input)          deriv = 1 - (output**2)
- Softmax  temp = np.exp(input)             temp = (output - true)
           output /= np.sum(temp)           output = temp/len(true)

A convolution layer aggregates the kernels with sum pooling, mean pooling, or max pooling.  Max pooling is the most common.

When a neural network needs to use the same idea in mutliple places, endeavor to use the same weights in both places.
</pre></div>